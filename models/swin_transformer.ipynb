{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "YVXTj5LcJqdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKVeH5bEeA3k"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q transformers\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "import collections\n",
        "import copy\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import AutoFeatureExtractor, SwinForImageClassification\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77eCS9WTeEII"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/data/'\n",
        "!rm -rf $data_path\n",
        "!mkdir -p $data_path\n",
        "\n",
        "drive_path = '/content/drive/'\n",
        "drive.mount(drive_path)\n",
        "project_path = os.path.join(drive_path, 'MyDrive/dissertation')  #project folder\n",
        "kaggle_path = os.path.join(project_path, '.kaggle')\n",
        "\n",
        "kaggle_credentials_path = '/root/.kaggle'\n",
        "!mkdir -p $kaggle_credentials_path\n",
        "%cd $kaggle_credentials_path\n",
        "\n",
        "if not os.path.exists(os.path.join(kaggle_credentials_path, 'kaggle.json')):\n",
        "    if not os.path.exists(os.path.join(kaggle_path, 'kaggle.json')):\n",
        "        files.upload() #this will prompt you to update the json\n",
        "    shutil.copy(os.path.join(kaggle_path, 'kaggle.json'), kaggle_credentials_path)\n",
        "    \n",
        "!pip install -q kaggle\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!ls $kaggle_credentials_path\n",
        "!chmod 600 $kaggle_credentials_path/kaggle.json  # set permission\n",
        "!kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNEQn1gSeHKX"
      },
      "outputs": [],
      "source": [
        "%cd $data_path\n",
        "dataset_path = 'covid19pneumonianormal-covidx-cxr2.zip'\n",
        "!kaggle datasets download -d inesfilipe/covid19pneumonianormal-covidx-cxr2\n",
        "!unzip -n $dataset_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lskdf4cheJxl"
      },
      "outputs": [],
      "source": [
        "class COVIDxDataset(Dataset):\n",
        "    def __init__(self, root, metadata, transform=None, target_transform=None, train=True):\n",
        "        self.root = root\n",
        "        self.metadata = metadata\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.class_to_idx = {label: i for i, label in enumerate(np.unique(metadata[:,2]))}\n",
        "        self.classes = sorted([label for label in self.class_to_idx])\n",
        "        self.targets = [self.class_to_idx[i] for i in metadata[:,2]]\n",
        "        self.imgs = [(os.path.join(self.root, img[2], img[1]), self.class_to_idx[img[2]]) for img in metadata]\n",
        "        self.train = train\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "    \n",
        "    def pil_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert(\"RGB\")\n",
        "    \n",
        "    def accimage_loader(self, path):\n",
        "        import accimage\n",
        "\n",
        "        try:\n",
        "            return accimage.Image(path)\n",
        "        except OSError:\n",
        "            return self.pil_loader(path)\n",
        "\n",
        "    def default_loader(self, path):\n",
        "        from torchvision import get_image_backend\n",
        "\n",
        "        if get_image_backend() == \"accimage\":\n",
        "            return self.accimage_loader(path)\n",
        "        else:\n",
        "            return self.pil_loader(path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.imgs[idx]\n",
        "        img_path = sample[0]\n",
        "        target = sample[1]\n",
        "\n",
        "        image = self.default_loader(img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        if self.target_transform:\n",
        "            target = self.transform(target)\n",
        "\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTx0ATO32pPu"
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n",
        "        \"train\": transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        \"test\": transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "}\n",
        "\n",
        "train_metadata = np.genfromtxt(\"train.txt\", delimiter=\" \", dtype=None, encoding=\"utf-8\")\n",
        "test_metadata = np.genfromtxt(\"test.txt\", delimiter=\" \", dtype=None, encoding=\"utf-8\")\n",
        "\n",
        "training_dataset = COVIDxDataset(\"train\", train_metadata, data_transforms[\"train\"], train=True)\n",
        "test_dataset = COVIDxDataset(\"test\", test_metadata, data_transforms[\"test\"], train=False)\n",
        "\n",
        "def make_sampler(dataset, balance=True):\n",
        "    def make_weights_for_balanced_classes(dataset):\n",
        "        target_count = collections.Counter(dataset.targets)\n",
        "        total = sum(target_count.values())\n",
        "        target_weight = {k : sum(target_count.values()) / v for (k, v) in target_count.items()}\n",
        "        weight = [0.] * total\n",
        "    \n",
        "        for index, value in enumerate(dataset.imgs):\n",
        "            weight[index] = target_weight[value[1]]\n",
        "        return weight\n",
        "  \n",
        "    if balance:\n",
        "        weights = torch.DoubleTensor(make_weights_for_balanced_classes(dataset))\n",
        "        return torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n",
        "    return None\n",
        "\n",
        "def make_loader(dataset, batch_size, sampler=None):\n",
        "    if sampler == None:\n",
        "        return torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True, \n",
        "                                           num_workers=2)\n",
        "    return torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                       batch_size=batch_size, \n",
        "                                       sampler=sampler,\n",
        "                                       pin_memory=True, \n",
        "                                       num_workers=2)\n",
        "\n",
        "train_loader = make_loader(training_dataset, batch_size=64)\n",
        "test_loader = make_loader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw5b3dNbibxi"
      },
      "outputs": [],
      "source": [
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "model = SwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", \n",
        "                                                       num_labels=len(test_dataset.classes),\n",
        "                                                       id2label={y : x for x,y in test_dataset.class_to_idx.items()},\n",
        "                                                       label2id=test_dataset.class_to_idx,\n",
        "                                                       ignore_mismatched_sizes=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEP87kTJo9EX"
      },
      "outputs": [],
      "source": [
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_accu = {}\n",
        "train_losses = {}"
      ],
      "metadata": {
        "id": "bpf6efhSS_eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  print(f\"Epoch: {epoch}\")\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for idx, (images, labels) in enumerate(tqdm(train_loader)):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    optim.zero_grad()\n",
        "    outputs = model(pixel_values=images, labels=labels)\n",
        "    loss = criterion(outputs.logits, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    total += labels.shape[0]\n",
        "    predicted = outputs.logits.argmax(-1)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "    if idx % 100 == 0:\n",
        "      print(f\"Total = {total}\")\n",
        "      print(f\"Loss after {idx} steps: {loss.item()}\")\n",
        "      print(f\"Accuracy after {idx} steps: {correct / total}\")\n",
        "  \n",
        "  train_loss = running_loss / len(train_loader)\n",
        "  accu = 100. * correct / total\n",
        "  \n",
        "  train_accu[epoch] = accu\n",
        "  train_losses[epoch] = train_loss\n",
        "  print('Train Loss: %.3f | Accuracy: %.3f' % (train_loss, accu))"
      ],
      "metadata": {
        "id": "AP8TB3u2S_Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_accu = {}\n",
        "eval_losses = {}\n",
        "confusion_matrices = {}\n",
        "binary_confusion_matrices = {}\n",
        "per_class_metrics = {}\n",
        "micro_metrics = {}\n",
        "macro_metrics = {}\n",
        "weighted_metrics = {}\n",
        "total_preds = {}\n",
        "total_labels = {}"
      ],
      "metadata": {
        "id": "aL-HzRjjS_DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr2ZE_Pa39b5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "def test(epoch):\n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for _, (images, labels) in enumerate(tqdm(test_loader)):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(pixel_values=images, labels=labels)\n",
        "\n",
        "      loss = criterion(outputs.logits, labels)\n",
        "      running_loss += loss.item()\n",
        "      total += labels.shape[0]\n",
        "      predicted = outputs.logits.argmax(-1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "      if epoch in total_preds:\n",
        "        total_preds[epoch] = torch.cat((total_preds[epoch], predicted))\n",
        "      else:\n",
        "        total_preds[epoch] = predicted\n",
        "      \n",
        "      if epoch in total_labels:\n",
        "        total_labels[epoch] = torch.cat((total_labels[epoch], labels))\n",
        "      else:\n",
        "        total_labels[epoch] = labels\n",
        "      \n",
        "    test_loss = running_loss / len(test_loader)\n",
        "    accu = 100. * correct / total\n",
        "    \n",
        "    eval_accu[epoch] = accu\n",
        "    eval_losses[epoch] = test_loss\n",
        "    total_labels[epoch] = total_labels[epoch].cpu()\n",
        "    total_preds[epoch] = total_preds[epoch].cpu()\n",
        "    confusion_matrices[epoch] = confusion_matrix(total_labels[epoch], total_preds[epoch], labels=[0,1,2])\n",
        "    binary_confusion_matrices[epoch] = multilabel_confusion_matrix(total_labels[epoch], total_preds[epoch], labels=[0,1,2])\n",
        "\n",
        "    per_class_metrics[epoch] = precision_recall_fscore_support(total_labels[epoch], total_preds[epoch], labels=[0,1,2])\n",
        "    micro_metrics[epoch] = precision_recall_fscore_support(total_labels[epoch], total_preds[epoch], labels=[0,1,2], average='micro')\n",
        "    macro_metrics[epoch] = precision_recall_fscore_support(total_labels[epoch], total_preds[epoch], labels=[0,1,2], average='macro')\n",
        "    weighted_metrics[epoch] = precision_recall_fscore_support(total_labels[epoch], total_preds[epoch], labels=[0,1,2], average='weighted')\n",
        "\n",
        "    print(f\"Total = {total}\")\n",
        "    print(f\"Accuracy {correct / total}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "  train(epoch)\n",
        "test(epochs+1)"
      ],
      "metadata": {
        "id": "kSUUZlyoLvvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_accu)\n",
        "print(train_losses)\n",
        "print(eval_accu)\n",
        "print(eval_losses)\n",
        "print(confusion_matrices)\n",
        "print(binary_confusion_matrices)\n",
        "print(per_class_metrics)\n",
        "print(micro_metrics)\n",
        "print(macro_metrics)\n",
        "print(weighted_metrics)\n",
        "print(test_dataset.class_to_idx)\n",
        "print(total_preds)\n",
        "print(total_labels)"
      ],
      "metadata": {
        "id": "UroNL1tE9e96"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "swin_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}